
# ğŸ•·ï¸ Spidey: The Web Crawler

Spidey is a lightweight, extensible Python-based web crawler built for rapid deployment and real-time data extraction. It supports intelligent link-following, language detection, readability scoring, duplicate content detection, and structured data export â€” all with a simple SQLite backend.

<img width="1363" height="768" alt="image" src="https://github.com/user-attachments/assets/4dda8e84-e745-449a-a9c4-2a0c15ff9baa" />


---

## ğŸš€ Features

- ğŸ”— Extracts and follows internal links
- ğŸ§  Detects language and readability of pages
- ğŸ§¹ Cleans HTML to extract useful content
- ğŸ“„ Stores pages, links, and sessions in SQLite
- ğŸ“¦ Exports data as JSON
- ğŸ” Relative link support (e.g., Wikipedia)
- ğŸ§µ Multithreaded crawling

---

## ğŸ“¦ Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```

---

## ğŸ› ï¸ Usage

### Run the crawler:

```bash
python database_schema.py
python app.py
```

It will start a crawl from the given URL and save the session to the SQLite database.

---

## âš™ï¸ Project Structure

```
Spidey-The-Web-Crawler/
â”‚
â”œâ”€â”€ crawler.py             # Main crawler logic
â”œâ”€â”€ app.py 
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ database/              # SQLite database file
    |
    â”œâ”€â”€crawler.db
â”œâ”€â”€ exports/               # JSON export output
â”œâ”€â”€ static/              
    |
    â”œâ”€â”€script.js
    â”œâ”€â”€style.css
â”œâ”€â”€ templates/              
    |
    â”œâ”€â”€index.html
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md              
```

---

## ğŸ§  Future Ideas

- Export to CSV
- Add media/video/document crawling
- Frontend for live progress (Flask/UI)
- Scheduler to auto-run crawls

---

## ğŸ“„ License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file.

---

## ğŸ’¬ Feedback

Found a bug? Have a feature request? Open an [issue](https://github.com/hridyansh5492/Spidey-The-Web-Crawler/issues) or drop a â­ if you like the project!
